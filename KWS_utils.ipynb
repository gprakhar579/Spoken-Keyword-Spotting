{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "KWS_utils.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "from sklearn import svm\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import f1_score\n",
        "import os\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import numpy as np\n",
        "from scipy.io import wavfile\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "from sklearn.metrics import accuracy_score, recall_score, matthews_corrcoef\n",
        "from sklearn.metrics import precision_score, f1_score, confusion_matrix\n",
        "from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay\n",
        "from python_speech_features import logfbank\n",
        "import tarfile\n",
        "import requests\n",
        "import pandas as pd\n",
        "from path import Path"
      ],
      "outputs": [],
      "metadata": {
        "id": "Mq3MD1UvjvrH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "NUM_CLASSES = 31\n",
        "AUDIO_SR = 16000\n",
        "AUDIO_LENGTH = 16000\n",
        "LIBROSA_AUDIO_LENGTH = 22050\n",
        "EPOCHS = 25\n",
        "categories = {\n",
        "    \"stop\": 0,\n",
        "    \"nine\": 1,\n",
        "    \"off\": 2,\n",
        "    \"four\": 3,\n",
        "    \"right\": 4,\n",
        "    \"eight\": 5,\n",
        "    \"one\": 6,\n",
        "    \"bird\": 7,\n",
        "    \"dog\": 8,\n",
        "    \"no\": 9,\n",
        "    \"on\": 10,\n",
        "    \"seven\": 11,\n",
        "    \"cat\": 12,\n",
        "    \"left\": 13,\n",
        "    \"three\": 14,\n",
        "    \"tree\": 15,\n",
        "    \"bed\": 16,\n",
        "    \"zero\": 17,\n",
        "    \"happy\": 18,\n",
        "    \"sheila\": 19,\n",
        "    \"five\": 20,\n",
        "    \"down\": 21,\n",
        "    \"marvin\": 22,\n",
        "    \"six\": 23,\n",
        "    \"up\": 24,\n",
        "    \"wow\": 25,\n",
        "    \"house\": 26,\n",
        "    \"go\": 27,\n",
        "    \"yes\": 28,\n",
        "    \"two\": 29,\n",
        "    \"_background_noise_\": 30,\n",
        "}\n",
        "inv_categories = {\n",
        "    0: \"stop\",\n",
        "    1: \"nine\",\n",
        "    2: \"off\",\n",
        "    3: \"four\",\n",
        "    4: \"right\",\n",
        "    5: \"eight\",\n",
        "    6: \"one\",\n",
        "    7: \"bird\",\n",
        "    8: \"dog\",\n",
        "    9: \"no\",\n",
        "    10: \"on\",\n",
        "    11: \"seven\",\n",
        "    12: \"cat\",\n",
        "    13: \"left\",\n",
        "    14: \"three\",\n",
        "    15: \"tree\",\n",
        "    16: \"bed\",\n",
        "    17: \"zero\",\n",
        "    18: \"happy\",\n",
        "    19: \"sheila\",\n",
        "    20: \"five\",\n",
        "    21: \"down\",\n",
        "    22: \"marvin\",\n",
        "    23: \"six\",\n",
        "    24: \"up\",\n",
        "    25: \"wow\",\n",
        "    26: \"house\",\n",
        "    27: \"go\",\n",
        "    28: \"yes\",\n",
        "    29: \"two\",\n",
        "    30: \"_background_noise_\",\n",
        "}\n",
        "\n",
        "# Sheila model\n",
        "INPUT_SHAPE = (99, 40)\n",
        "TARGET_SHAPE = (99, 40, 1)\n",
        "PARSE_PARAMS = (0.025, 0.01, 40)\n",
        "filters = [16, 32, 64, 128, 256]\n",
        "\n",
        "DROPOUT = 0.25\n",
        "KERNEL_SIZE = (3, 3)\n",
        "POOL_SIZE = (2, 2)\n",
        "DENSE_1 = 512\n",
        "DENSE_2 = 256\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "PATIENCE = 5\n",
        "LEARNING_RATE = 0.001"
      ],
      "outputs": [],
      "metadata": {
        "id": "6rNvf-OLkG_6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "def getDataset(df, batch_size, cache_file=None, shuffle=True, parse_param=PARSE_PARAMS, scale=False):\n",
        "    \"\"\"\n",
        "    Creates a Tensorflow Dataset containing filterbanks, labels\n",
        "    :param df: Dataframe with filenames and labels\n",
        "    :param batch_size: Batch size of the input\n",
        "    :param cache_file: Whether to cache the dataset during run\n",
        "    :param shuffle: Whether to shuffle the dataset\n",
        "    :param parse_param: Window parameters\n",
        "    :param scale: Whether to scale filterbank levels\n",
        "    :return: TF Dataset, Steps per epoch\n",
        "    \"\"\"\n",
        "\n",
        "    data = tf.data.Dataset.from_tensor_slices((df[\"files\"].tolist(), df[\"labels\"].tolist()))\n",
        "\n",
        "    data = data.map(\n",
        "        lambda filename, label: tuple(\n",
        "            tf.py_function(_parse_fn, inp=[filename, label, parse_param, scale], Tout=[tf.float32, tf.int32])\n",
        "        ),\n",
        "        num_parallel_calls=os.cpu_count(),\n",
        "    )\n",
        "\n",
        "    if cache_file:\n",
        "        data = data.cache(\"../input/\" + cache_file)\n",
        "\n",
        "    if shuffle:\n",
        "        data = data.shuffle(buffer_size=df.shape[0])\n",
        "\n",
        "    data = data.batch(batch_size).prefetch(buffer_size=1)\n",
        "    steps = df.shape[0] // batch_size\n",
        "\n",
        "    return data, steps\n",
        "\n",
        "\n",
        "def plot_history(history):\n",
        "    \"\"\"\n",
        "    Plots and saves training history\n",
        "    :param history: Training history\n",
        "    :param model_name: Model name\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "\n",
        "    sns.set()\n",
        "\n",
        "    loss = history.history[\"loss\"]\n",
        "    val_loss = history.history[\"val_loss\"]\n",
        "\n",
        "    acc = history.history[\"sparse_categorical_accuracy\"]\n",
        "    val_acc = history.history[\"val_sparse_categorical_accuracy\"]\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8))\n",
        "\n",
        "    ax1.plot(loss, label=\"Training\")\n",
        "    ax1.plot(val_loss, label=\"Validation\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "    ax1.set_title(\"Model loss\")\n",
        "    ax1.set_xlabel(\"Epoch\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2.plot(acc, label=\"Training\")\n",
        "    ax2.plot(val_acc, label=\"Validation\")\n",
        "    ax2.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "    ax2.set_title(\"Accuracy\")\n",
        "    ax2.set_xlabel(\"Epoch\")\n",
        "    ax2.set_ylabel(\"Accuracy\")\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"model_training.png\", dpi=300)\n",
        "    fig.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "gUGig2yzjxxv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "def downloadData(data_path=\"/input/speech_commands/\"):\n",
        "    \"\"\"\n",
        "    Downloads Google Speech Commands dataset (version0.01)\n",
        "    :param data_path: Path to download dataset\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "\n",
        "    dataset_path = Path(os.path.abspath(__file__)).parent.parent + data_path\n",
        "\n",
        "    datasets = [\"train\", \"test\"]\n",
        "    urls = [\n",
        "        \"http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz\",\n",
        "        \"http://download.tensorflow.org/data/speech_commands_test_set_v0.01.tar.gz\",\n",
        "    ]\n",
        "\n",
        "    for dataset, url in zip(datasets, urls):\n",
        "        dataset_directory = dataset_path + dataset\n",
        "\n",
        "        # Check if we need to extract the dataset\n",
        "        if not os.path.isdir(dataset_directory):\n",
        "            os.makedirs(dataset_directory)\n",
        "            file_name = dataset_path + dataset + \".tar.gz\"\n",
        "\n",
        "            # Check if the dataset has been downloaded, else download it\n",
        "            if os.path.isfile(file_name):\n",
        "                print(\"{} already downloaded. Skipping download.\".format(file_name))\n",
        "            else:\n",
        "                print(\"Downloading '{}' into '{}' file\".format(url, file_name))\n",
        "\n",
        "                data_request = requests.get(url)\n",
        "                with open(file_name, \"wb\") as file:\n",
        "                    file.write(data_request.content)\n",
        "\n",
        "            # Extract downloaded file\n",
        "            print(\"Extracting {} into {}\".format(file_name, dataset_directory))\n",
        "\n",
        "            if file_name.endswith(\"tar.gz\"):\n",
        "                tar = tarfile.open(file_name, \"r:gz\")\n",
        "                tar.extractall(path=dataset_directory)\n",
        "                tar.close()\n",
        "            else:\n",
        "                print(\"Unknown format.\")\n",
        "        else:\n",
        "            print(f\"{dataset} data setup complete.\")\n",
        "\n",
        "    print(\"Input data setup successful.\")\n",
        "\n",
        "\n",
        "def getDataDict(data_path=\"/input/speech_commands/\"):\n",
        "    \"\"\"\n",
        "    Creates a dictionary with train, test, validate and test file names and labels.\n",
        "    :param data_path: Path to the downloaded dataset\n",
        "    :return: Dictionary\n",
        "    \"\"\"\n",
        "\n",
        "    data_path = Path(os.path.abspath(__file__)).parent.parent + data_path\n",
        "\n",
        "    # Get the validation files\n",
        "    validation_files = open(data_path + \"train/validation_list.txt\").read().splitlines()\n",
        "    validation_files = [data_path + \"train/\" + file_name for file_name in validation_files]\n",
        "\n",
        "    # Get the dev files\n",
        "    dev_files = open(data_path + \"train/testing_list.txt\").read().splitlines()\n",
        "    dev_files = [data_path + \"train/\" + file_name for file_name in dev_files]\n",
        "\n",
        "    # Find train_files as allFiles - {validation_files, dev_files}\n",
        "    all_files = []\n",
        "    for root, dirs, files in os.walk(data_path + \"train/\"):\n",
        "        all_files += [root + \"/\" + file_name for file_name in files if file_name.endswith(\".wav\")]\n",
        "\n",
        "    train_files = list(set(all_files) - set(validation_files) - set(dev_files))\n",
        "\n",
        "    # Get the test files\n",
        "    test_files = list()\n",
        "    for root, dirs, files in os.walk(data_path + \"test/\"):\n",
        "        test_files += [root + \"/\" + file_name for file_name in files if file_name.endswith(\".wav\")]\n",
        "\n",
        "    # Get labels\n",
        "    validation_file_labels = [getLabel(wav) for wav in validation_files]\n",
        "    dev_file_labels = [getLabel(wav) for wav in dev_files]\n",
        "    train_file_labels = [getLabel(wav) for wav in train_files]\n",
        "    test_file_labels = [getLabel(wav) for wav in test_files]\n",
        "\n",
        "    # Create dictionaries containing (file, labels)\n",
        "    trainData = {\"files\": train_files, \"labels\": train_file_labels}\n",
        "    valData = {\"files\": validation_files, \"labels\": validation_file_labels}\n",
        "    devData = {\"files\": dev_files, \"labels\": dev_file_labels}\n",
        "    testData = {\"files\": test_files, \"labels\": test_file_labels}\n",
        "\n",
        "    dataDict = {\"train\": trainData, \"val\": valData, \"dev\": devData, \"test\": testData}\n",
        "\n",
        "    return dataDict\n",
        "\n",
        "\n",
        "def getLabel(file_name):\n",
        "    \"\"\"\n",
        "    Extract the label from its file path\n",
        "    :param file_name: File name\n",
        "    :return: Class label\n",
        "    \"\"\"\n",
        "\n",
        "    category = file_name.split(\"/\")[-2]\n",
        "    label = categories.get(category, categories[\"_background_noise_\"])\n",
        "\n",
        "    return label\n",
        "\n",
        "\n",
        "def getDataframe(data, include_unknown=False):\n",
        "    \"\"\"\n",
        "    Create a dataframe from a Dictionary and remove _background_noise_\n",
        "    :param data: Data dictionary\n",
        "    :param include_unknown: Whether to include unknown sounds or not\n",
        "    :return: Dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df[\"category\"] = df.apply(lambda row: inv_categories[row[\"labels\"]], axis=1)\n",
        "\n",
        "    if not include_unknown:\n",
        "        df = df.loc[df[\"category\"] != \"_background_noise_\", :]\n",
        "\n",
        "    return df\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "cfe99mxdj82F"
      }
    }
  ]
}